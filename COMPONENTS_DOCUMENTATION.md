# Connect 4 Alpha Zero実装 - コンポーネント詳細説明書

## プロジェクト概要

このプロジェクトは、DeepMindのAlpha Zeroアルゴリズムを使用してConnect 4（四目並べ）ゲームを学習するPython実装です。

### 主な特徴
- **bitboardエンコーディング**：6×7ボードに特化した高速な64ビット整数表現
- **並列自己対戦**：CPU並列化によるゲーム生成
- **ニューラルネットワーク学習**：GPU対応（利用可能な場合）
- **柔軟なネットワークアーキテクチャ**：ResNetまたはDenseNetを選択可能
- **ELOレーティング**：学習進捗の定量的評価

## システムアーキテクチャ

### 1. コアコンポーネント層

#### 1.1 ゲーム環境層

**`Game_bitboard.py`**
- **役割**：Connect 4ゲームのルールとゲーム状態を管理
- **主要機能**：
  - bitboardエンコーディングによる高速な盤面表現
  - 合法手の生成
  - 勝敗判定
  - 盤面状態の変換（ニューラルネットワーク入力用）
- **ビットボード表現**：
  ```
  5  13 21  29  37  45  53
  4  12 20  28  36  44  52
  3  11 19  27  35  43  51
  2  10 18  26  34  42  50
  1  9  17  25  33  41  49
  0  8  16  24  32  40  48
  ```

#### 1.2 探索アルゴリズム層

**`MCTS.py`**
- **役割**：純粋なMonte Carlo Tree Search実装
- **主要機能**：
  - UCT（Upper Confidence Bound for Trees）による木探索
  - ランダムロールアウトポリシー
  - 勝利手・防御手の認識オプション
- **使用場面**：ELOレーティング計算時の基準プレイヤー

**`MCTS_NN.py`**
- **役割**：ニューラルネットワークガイド付きMCTS
- **主要機能**：
  - PUCT（Polynomial Upper Confidence Trees）による探索
  - ニューラルネットワークによる盤面評価と手の確率予測
  - Dirichletノイズによる探索の多様性確保
  - ルートノードでの温度パラメータ制御

#### 1.3 ニューラルネットワーク層

**`ResNet.py`**
- **役割**：ゲーム状態評価用のニューラルネットワークモデル
- **アーキテクチャ選択**：
  1. **ResNet**（推奨）：
     - 残差ブロックによる深い学習
     - デフォルト：1ブロック、256フィルタ、4×4カーネル
     - 約120万パラメータ
  2. **DenseNet**：
     - 完全結合層による実装
     - 2つの隠れ層（デフォルト1024ユニット）
     - 約200万パラメータ
- **出力**：
  - **価値ヘッド**：現在のプレイヤーの勝率（-1から1）
  - **ポリシーヘッド**：各列への着手確率（7次元）

### 2. 学習・制御層

#### 2.1 メイン制御

**`Main.py`**
- **役割**：学習ループの制御
- **処理フロー**：
  1. ニューラルネットワークの初期化/読み込み
  2. 自己対戦によるデータ生成
  3. 経験リプレイによるモデル改善
  4. 改善評価（前世代との対戦）
  5. ELOレーティング計算
  6. 収束判定

**`main_functions.py`**
- **役割**：学習に必要な主要機能の実装
- **機能群**：
  - `generate_self_play_data()`：並列自己対戦
  - `improve_model_resnet()`：ニューラルネットワーク学習
  - `play_v1_against_v2()`：モデル改善評価
  - `geteloratings()`：ELOレーティング計算
  - `printstates()`：重要局面での評価値表示

#### 2.2 設定管理

**`config.py`**
- **役割**：全システムパラメータの集中管理
- **主要設定**：
  - ボードサイズ（6×7固定）
  - MCTS設定（シミュレーション数、CPUCT値、温度）
  - ニューラルネットワーク設定（アーキテクチャ、学習率）
  - 学習設定（バッチサイズ、エポック数）
  - 評価設定（対戦数、改善閾値）

### 3. 評価・分析ツール層

**`elorating.py`**
- **役割**：学習済みモデルのELOレーティング測定
- **機能**：様々なシミュレーション数のMCTSとの対戦

**`pre_compute_elo_ratings.py`**
- **役割**：純粋MCTSプレイヤーのELOスケール事前計算
- **基準**：ランダムプレイヤーのELOを0に設定

**`Draw_Elo.py`**
- **役割**：ELOレーティングの推移をグラフ化
- **出力**：学習曲線の可視化

### 4. ユーザーインターフェース層

**`play_against_human.py`**
- **役割**：人間対AIの対戦インターフェース
- **機能**：
  - 先手・後手の選択
  - AIの思考過程表示（評価値、訪問回数）
  - 最適手との比較（http://connect4.gamesolver.org/）

## データフローと相互作用

### 学習サイクル

```
1. 自己対戦フェーズ
   Main.py → main_functions.generate_self_play_data()
           → 並列プロセス起動
           → MCTS_NN + Game_bitboard
           → 対戦データ生成

2. 学習フェーズ
   生成データ → main_functions.improve_model_resnet()
             → ResNet.ResNet_Training
             → モデル更新

3. 評価フェーズ
   新モデル vs 旧モデル → main_functions.play_v1_against_v2()
                      → 改善判定
                      → モデル保存/破棄

4. ELO計算フェーズ
   最良モデル vs MCTS → main_functions.geteloratings()
                     → 学習進捗の定量化
```

### コンポーネント間の依存関係

```
┌─────────────┐
│   Main.py   │ ← エントリーポイント
└──────┬──────┘
       │
┌──────┴───────────────┐
│ main_functions.py    │ ← 制御ロジック
└──────┬───────────────┘
       │
┌──────┴────────┬─────────────┬──────────────┐
│ MCTS_NN.py   │ ResNet.py   │ config.py    │
└──────┬────────┴─────────────┴──────────────┘
       │
┌──────┴──────────┐
│ Game_bitboard.py │ ← ゲームルール
└─────────────────┘
```

## 重要な設計上の工夫

### 1. 長期ゲーム優遇メカニズム
- **問題**：先手必勝のゲーム性による学習の振動
- **解決**：`favor_long_games`パラメータによる報酬調整
  - 勝者：早い勝利ほど高報酬
  - 敗者：遅い敗北ほど高報酬（負の値が小さくなる）

### 2. 温度制御による探索と活用のバランス
- **自己対戦時**：序盤は探索重視（τ=1）、終盤は最善手選択（τ→0）
- **評価時**：常に最善手選択（τ=0）

### 3. データ拡張
- 盤面の左右反転による学習データの倍増
- 対称性を利用した効率的な学習

### 4. 並列化戦略
- CPU：自己対戦の並列実行
- GPU：ニューラルネットワークの学習

## 使用方法

### 学習の開始
```python
python Main.py
```

### 人間との対戦
```python
python play_against_human.py
```

### ELOレーティング測定
```python
python elorating.py
```

## 学習の収束
- 約150イテレーション（40CPU、1GPU使用で数時間）
- ELOレーティング1800以上
- 重要局面での正確な評価値

## 出力ファイル
- `best_model_resnet.pth`：最良のニューラルネットワークモデル
- `NN_elo_ratings.png`：学習曲線グラフ